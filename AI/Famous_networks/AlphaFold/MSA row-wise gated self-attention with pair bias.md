The MSA representation is a 3D array of shape (Nclust ;Nres ; 256), the multi head attention layer takes in an (m,n) matrix, so in this case we can think of the msa as a (Nclust ;Nres) matrix of vectors of 256 dimensions representing the aminoacids and other relevant info about that position in that cluster. 
In the attention algorithm you have sets of keys and values which are sets of vectors, and the return is a linear combination of the values acording to the similarity between the query and every key, and concatenate for every query,  in our case each vector is actually a vector of vectors, in the case of row wise the vectors are going to be a particular position in the alignment, its a vector of vectors since its going to have Ncluster sequences to check that position and the aa in that position and other info is described by a 256 vector. the query key and value vectors are going to be of that shape and their dot products are the sums of the inner dot products of the residues or equivalently the dot products of their flattened versions.
this leaves us after the query key dots with Nheads of (Nres,Nres) matrices, relating each position in the alignment with every other, the same shape of the pair representation and since the network learns to use the evolutionary information from the msa to the interactions, this is a good point to add the information from the pair representation to the msa representation, so its added there as a bias by applying a linear transformation from 
(Nres;Nres,128) to (Nres;Nres,Nheads).
Finally as a last step that differs from the basic multihead attention  layer theres another tensor calculated from the MSA, G which is the multiplied pointwise after the attention algorithm.
[[Self-Attention]]
![[MSA row-wise gated self-attention with pair bias.canvas]]